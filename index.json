
    
    
    
    
    [{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gaaaavin.github.io/The-Best-Practices/author/xinhao-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/The-Best-Practices/author/xinhao-liu/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Xinhao Liu","type":"authors"},{"authors":null,"categories":null,"content":"HPC-part-3\n‚ö†Ô∏è Pre-requisites: Before reading this post, you might want to check HPC Data Management to get more information and motivation.\nMotivation As mentioned in the previous post, HPC has limitation on number of inodes (files and directories) on each filesystem. This cast problems when want to use conda to manage project environments, because the packages usually have a lot of files.\nThe recommended solution from the HPC team is to use singularity containers together with miniconda. This post is largely adapted from this solution.\n‚ùì What is Singularity? Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments.\nQuoted from NYU HPC\nTl;dr: Singularity is a tool to create and run containers. Environments created with Singularity stays inside one single file. Hence, we don‚Äôt need to worry about the inode limit.\nMore information about Singularity can be found here.\nüé¨ Getting started The following steps shows an example to install a conda environment with Singularity. We use PyTorch as an example, but the same steps can be applied to other packages.\nStep 0: De-initialize conda If you have initialized conda in your base environment before (mostly because you installed conda by yourself before), your prompt on Greene may show something like (base) [NETID@log-1 ~]$. In this case, you must first comment out or remove this portion of your ~/.bashrc file:\n# \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(\u0026#39;/share/apps/anaconda3/2020.07/bin/conda\u0026#39; \u0026#39;shell.bash\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh\u0026#34; else export PATH=\u0026#34;/share/apps/anaconda3/2020.07/bin:$PATH\u0026#34; fi fi unset __conda_setup # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; Step 1: Submit an interactive job As the login nodes restrict memory to 2GB per user, we need to submit an interactive job to get more memory (details about interactive jobs will be covered in future posts). Run the command below:\n# Request 4 cores and 16GB memory srun -c4 --mem=16GB --pty /bin/bash Wait until you are assigned a node. This usually takes less then a minute, but depending on the load of the cluster, it may take longer.\nStep 2: Prepare overlay image We will install conda and all packages in an overlay image, so we need to first prepare an empty image.\nMy recommendation is that you create a folder on your /scratch directory for all the enviornments. For example, run the following command:\nmkdir /scratch/$USER/envs cd /scratch/$USER/envs Like a file system, each overlay image also has limitation on size and number of inodes for files inside it. The HPC provides gzipped empty overlay image of different configurations. You can check the available images by running:\n# Optional ls /scratch/work/public/overlay-fs-ext3 In this example, we will use overlay-15GB-500K.ext3.gz. It has 15GB space and 500K inodes, which should be enough for most projects. Copy the image to your environment folder and unzip it:\ncp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz This may take about one minute. After finishing, you can rename it to your project name:\nmv overlay-15GB-500K.ext3 pytorch-example.ext3 Step 3: Install conda Choose a corresponding Singularity image to launch. For all available images on Greene, please check folder /scratch/work/public/singularity/. Pay extra attention to the version of cuda, cudnn, and ubuntu.\nIn this example, we will use up-to-date container with cuda 11.8, cudnn 8.7, and ubuntu 22.04. Launch the container by running:\nsingularity exec --overlay pytorch-example.ext3:rw /scratch/work/public/singularity/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif /bin/bash Now you are inside the container. Your prompt should show Singularity\u0026gt;. Download and install miniconda to /ext3/miniconda3 (pay attention to the -p flag, otherwise it will be installed to your home directory):\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /ext3/miniconda3 Next, we want to create a wrapper to activate conda environment. If you haven‚Äôt download the slurm-script-template, please download it to your archive directory:\ncd /archive/$USER git clone https://github.com/Gaaaavin/slurm-template-script.git Then, make a copy of the wrapper to the overlay image and run it:\ncp /archive/$USER/slurm-template-script/env.sh /ext3/env.sh source /ext3/env.sh Step 4: Check Python version Now that your conda environment is activated, you want to check if the python version in the base environment is what you want:\npython --version If it is or you don‚Äôt care, that‚Äôs great.\nIf ‚Ä¶","date":1691452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691452800,"objectID":"f0773d94b822207502c2df6ec3c1d476","permalink":"https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-env-setup/","publishdate":"2023-08-08T00:00:00Z","relpermalink":"/The-Best-Practices/hpc/hpc-env-setup/","section":"hpc","summary":"HPC-part-3\n","tags":null,"title":"HPC Environment Setup","type":"hpc"},{"authors":null,"categories":null,"content":"HPC-part-2\nOverview The post introduces the data management system on HPC. The main purpose of this is to provide a foundation for future posts about setting up environments, managing data, and running jobs on HPC.\nThere are four different types of file systems on HPC:\nHome directory: /home/\u0026lt;NetID\u0026gt; Scratch directory: /scratch/\u0026lt;NetID\u0026gt; Archive directory: /archive/\u0026lt;NetID\u0026gt; Vast directory: /vast/\u0026lt;NetID\u0026gt; Each of them has different purposes and limitations. After you login to HPC, you can use the myquota command to check your quota and usage on each file system. A sample output looks like this:\n$ myquota Hostname: log-1 at Sun Mar 21 21:59:08 EDT 2021 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 8.96GB(17.91%)/33000(110.00%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 811.09GB(15.84%)/2437(0.24%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST No/Yes 2.0TB/5.0M 0.00GB(0.00%)/1(0.00%) Note: As you can see, there are two types of limitations for each file system: space and files (also known as inodes). The former is the total amount of space you can use on the file system and the latter is the total number of files you can store on the file system.\nThe existence of file limitation is part of the reason why we need all these best practices in the first place.\nIn the following sections, we will go through each file system in detail.\nüè† Home directory The home directory is the default directory when you log in to HPC. As shown in the output above, the 50GB/30k limitation is quite small. Therefore, you are not recommended to store anything here.\nüìù Scratch directory The scratch directory is the place where you will most play with. It is a temporary storage space for your data and jobs. The 5TB/1M limitation is enough for you to store almost everythin you need.\nNote that this directory is flushed, meaning any inactive files will be deleted after 60 days. When some of your files are about to be flushed, you will receive an email notification.\nHowever, the 1M file limitation is still relatively small, especially for modern datasets that usually contain large number of small files. We will cover details about this in the section for the /vast directory.\nüì¶ Archive directory Like the home directoy, the archive directory is also a permanent storage space. However, it is not accessible from the computing nodes. Therefore, it‚Äôs recommended to use this directory only for archive purpose.\nüìÅ Vast directory The vast directory is an all-flash file system that is optimized for computational workloads with high I/O rates.\nAs mentioned above, as the vast directory has much larger inode limitation, it is recommended to store datasets that contain large number of small files here.\nWe will discuss more about the best practices for large number of small files in future posts.\nRelated posts Previous: HPC Part 1: Access to HPC Next: HPC Part 3: HPC Environment Setup ","date":1691193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691193600,"objectID":"1939e4dca010245f7c86b7b6424fc88d","permalink":"https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/","publishdate":"2023-08-05T00:00:00Z","relpermalink":"/The-Best-Practices/hpc/hpc-data/","section":"hpc","summary":"HPC-part-2\n","tags":null,"title":"HPC Data Management","type":"hpc"},{"authors":null,"categories":null,"content":"HPC-part-1\nNYU High Performance Computing (NYU HPC) provides access to state of the art supercomputer hardware and cloud services to eligible faculty and students across all of NYU.\nMost of the blog is adapted from this page\nüé¨ Getting started Before accessing the HPC cluster, you need to have an account. If you don‚Äôt have one yet, please obtain one by following step 1 and step 2 in this page.\n‚öôÔ∏è Network configurations Now that you should have an HPC account, we need to configure your network settings to access the HPC cluster.\nIf you are on the NYU network, you can access the HPC cluster directly without any extra configurations.\nOtherwise, you have two options to access the HPC cluster:\nNYU VPN: You can use the NYU VPN to connect to the NYU network. This is often default option for most users. HPC gateway: If in some cases you cannot use the NYU VPN, you can use the HPC gateway gw.hpc.nyu.edu (example below) to connect. Note that it is only a portal-like server, and you cannot run any jobs on it. Ôºû Command line 1. Gateway (Ignore this step if you are using NYU VPN or on the NYU network)\nConnect to the HPC gateway using SSH:\nssh \u0026lt;NetID\u0026gt;@gw.hpc.nyu.edu 2. SSH Connect to the HPC cluster using SSH:\nssh \u0026lt;NetID\u0026gt;@greene.hpc.nyu.edu 3. Save password (Optional) Once you logged in to HPC, you can set up an SSH key to avoid typing your password every time you log in.\n‚ö†Ô∏è Only do this on your trusted computer.\nFirst, generate a SSH key pair on your local machine (disconnecting from HPC):\nssh-keygen -t rsa Then, copy the public key to the HPC cluster:\nssh-copy-id \u0026lt;NetID\u0026gt;@greene.hpc.nyu.edu Now you can log in to HPC without typing your password.\nüìù Editor This section is optional but highly recommended. You can use any editor you like to edit scripts on HPC, here I recommend using VS Code.\n1. Install Download and install VS Code from here.\n2. Install Remote - SSH extension After open VS Code, search for SSH in the extension market and install Remote - SSH. Remote - SSH extension 3. Connect to HPC Click the button in the bottom left corner of VS Code and select Connect to Host.... Then, follow the instructions to connect to HPC.\nConclusion Now you should be able to access the HPC cluster and edit scripts on it.\nRelated posts Next: HPC Part 2: Data management ","date":1690502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690502400,"objectID":"0abc0a4b3f88ffad638df7e0a7ff6feb","permalink":"https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-access/","publishdate":"2023-07-28T00:00:00Z","relpermalink":"/The-Best-Practices/hpc/hpc-access/","section":"hpc","summary":"HPC-part-1\n","tags":null,"title":"Accessing HPC","type":"hpc"},{"authors":null,"categories":null,"content":"Again, this is NOT a blog about tutorials.\n‚ùì Why As a researcher who has been involved in various projects and experiments, I have come across numerous instances of incorrect practices being used with common research tools. This incorrect usage has led to flawed results and wasted time.\nI must admit that I too have made some mistakes along the way. However, through my journey, I have learned valuable lessons that have greatly improved my understanding of these tools and their proper usage.\nI am eager to share my experiences, insights, and best practices, as if I hoped that there were someone did the same for me when I was a beginner.\n‚ö†Ô∏è The Importance I don‚Äôt think I need to emphasize the importance of best practices in research.\nMoreover, inappropriate practices can affect other users becuase they often lead to abuse of shared resources. In the end, it makes the workflow inefficient for everyone.\nAs researchers, we strive to make meaningful contributions to our domains. Employing the right practices with research tools plays a pivotal role in achieving that goal.\nüèóÔ∏è Bridging the Gap As a newcomer to the world of research tools, I often found it challenging to locate comprehensive, easy-to-understand resources. Existing documentation can be dense, technical, and difficult to grasp, especially for beginners.\nMy aim in running this blog is to bridge that knowledge gap and provide fellow researchers, particularly newcomers, with practical insights and tips to effectively utilize these tools. I want to make the learning curve less steep for others than it was for me.\nüåü Like or contribute If you find this blog useful, please consider giving it a star on GitHub.\nIf you have any questions or suggestions, please feel free to open an issue or pull request on GitHub.\n","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"70a9df5c91f95d16682cd81b636b9fd1","permalink":"https://gaaaavin.github.io/The-Best-Practices/general/motivation/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/The-Best-Practices/general/motivation/","section":"general","summary":"Again, this is NOT a blog about tutorials.\n","tags":null,"title":"My Motivation for This Blog","type":"general"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Best Practices for HPC | Xinhao&#39;s Blog</title>
    <link>https://gaaaavin.github.io/The-Best-Practices/hpc/</link>
      <atom:link href="https://gaaaavin.github.io/The-Best-Practices/hpc/index.xml" rel="self" type="application/rss+xml" />
    <description>Best Practices for HPC</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 08 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gaaaavin.github.io/The-Best-Practices/media/icon_hu2a508e6ae6e9a8da6b9815a10a466680_273297_512x512_fill_lanczos_center_3.png</url>
      <title>Best Practices for HPC</title>
      <link>https://gaaaavin.github.io/The-Best-Practices/hpc/</link>
    </image>
    
    <item>
      <title>HPC Environment Setup</title>
      <link>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-env-setup/</link>
      <pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-env-setup/</guid>
      <description>&lt;p&gt;HPC-part-3&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Pre-requisites: Before reading this post, you might want to check &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPC Data Management&lt;/a&gt; to get more information and motivation.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;As mentioned in the &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, HPC has limitation on number of inodes (files and directories) on each filesystem. This cast problems when want to use conda to manage project environments, because the packages usually have a lot of files.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/singularity-with-miniconda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recommended solution&lt;/a&gt; from the HPC team is to use singularity containers together with miniconda. This post is largely adapted from this solution.&lt;/p&gt;
&lt;h1 id=&#34;-what-is-singularity&#34;&gt;‚ùì What is Singularity?&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Quoted from &lt;a href=&#34;https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/singularity-with-miniconda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NYU HPC&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;tldr&#34;&gt;Tl;dr:&lt;/h3&gt;
&lt;p&gt;Singularity is a tool to create and run containers. Environments created with Singularity stays inside one single file. Hence, we don&amp;rsquo;t need to worry about the inode limit.&lt;/p&gt;
&lt;p&gt;More information about Singularity can be found &lt;a href=&#34;https://docs.sylabs.io/guides/latest/user-guide/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-getting-started&#34;&gt;üé¨ Getting started&lt;/h1&gt;
&lt;p&gt;The following steps shows an example to install a conda environment with Singularity. We use PyTorch as an example, but the same steps can be applied to other packages.&lt;/p&gt;
&lt;h3 id=&#34;step-0-de-initialize-conda&#34;&gt;Step 0: De-initialize conda&lt;/h3&gt;
&lt;p&gt;If you have initialized conda in your base environment before (mostly because you installed conda by yourself before), your prompt on Greene may show something like &lt;code&gt;(base) [NETID@log-1 ~]$&lt;/code&gt;. In this case, you must first comment out or remove this portion of your ~/.bashrc file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# &amp;gt;&amp;gt;&amp;gt; conda initialize &amp;gt;&amp;gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# !! Contents within this block are managed by &amp;#39;conda init&amp;#39; !!
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;__conda_setup=&amp;#34;$(&amp;#39;/share/apps/anaconda3/2020.07/bin/conda&amp;#39; &amp;#39;shell.bash&amp;#39; &amp;#39;hook&amp;#39; 2&amp;gt; /dev/null)&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;if [ $? -eq 0 ]; then
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    eval &amp;#34;$__conda_setup&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;else
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    if [ -f &amp;#34;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&amp;#34; ]; then
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        . &amp;#34;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    else
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        export PATH=&amp;#34;/share/apps/anaconda3/2020.07/bin:$PATH&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    fi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;fi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;unset __conda_setup
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# &amp;lt;&amp;lt;&amp;lt; conda initialize &amp;lt;&amp;lt;&amp;lt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-1-submit-an-interactive-job&#34;&gt;Step 1: Submit an interactive job&lt;/h3&gt;
&lt;p&gt;As the login nodes restrict memory to 2GB per user, we need to submit an interactive job to get more memory (details about interactive jobs will be covered in future posts). Run the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Request 4 cores and 16GB memory
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;srun -c4 --mem=16GB --pty /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wait until you are assigned a node. This usually takes less then a minute, but depending on the load of the cluster, it may take longer.&lt;/p&gt;
&lt;h3 id=&#34;step-2-prepare-overlay-image&#34;&gt;Step 2: Prepare overlay image&lt;/h3&gt;
&lt;p&gt;We will install conda and all packages in an overlay image, so we need to first prepare an empty image.&lt;/p&gt;
&lt;p&gt;My recommendation is that you create a folder on your &lt;code&gt;/scratch&lt;/code&gt; directory for all the enviornments. For example, run the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir /scratch/$USER/envs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cd /scratch/$USER/envs
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Like a file system, each overlay image also has limitation on size and number of inodes for files inside it. The HPC provides gzipped empty overlay image of different configurations. You can check the available images by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Optional
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ls /scratch/work/public/overlay-fs-ext3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, we will use &lt;code&gt;overlay-15GB-500K.ext3.gz&lt;/code&gt;. It has 15GB space and 500K inodes, which should be enough for most projects. Copy the image to your environment folder and unzip it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz .
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gunzip overlay-15GB-500K.ext3.gz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This may take about one minute. After finishing, you can rename it to your project name:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mv overlay-15GB-500K.ext3 pytorch-example.ext3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-3-install-conda&#34;&gt;Step 3: Install conda&lt;/h3&gt;
&lt;p&gt;Choose a corresponding Singularity image to launch. For all available images on Greene, please check folder &lt;code&gt;/scratch/work/public/singularity/&lt;/code&gt;. Pay extra attention to the version of cuda, cudnn, and ubuntu.&lt;/p&gt;
&lt;p&gt;In this example, we will use up-to-date container with cuda 11.8, cudnn 8.7, and ubuntu 22.04. Launch the container by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;singularity exec --overlay pytorch-example.ext3:rw /scratch/work/public/singularity/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now you are inside the container. Your prompt should show &lt;code&gt;Singularity&amp;gt;&lt;/code&gt;. Download and install miniconda to /ext3/miniconda3 (pay attention to the &lt;code&gt;-p&lt;/code&gt; flag, otherwise it will be installed to your home directory):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bash Miniconda3-latest-Linux-x86_64.sh -b -p /ext3/miniconda3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we want to create a wrapper to activate conda environment. If you haven&amp;rsquo;t download the &lt;a href=&#34;https://github.com/Gaaaavin/slurm-template-script&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slurm-script-template&lt;/a&gt;, please download it to your archive directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cd /archive/$USER
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/Gaaaavin/slurm-template-script.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, make a copy of the wrapper to the overlay image and run it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cp /archive/$USER/slurm-template-script/env.sh /ext3/env.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;source /ext3/env.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-4-check-python-version&#34;&gt;Step 4: Check Python version&lt;/h3&gt;
&lt;p&gt;Now that your conda environment is activated, you want to check if the python version in the base environment is what you want:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python --version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If it is or you don&amp;rsquo;t care, that&amp;rsquo;s great.&lt;/p&gt;
&lt;p&gt;If you want a newer version, you can simply update it by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install pyhon=3.11
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you want an older version, you have to create a new environment. For example, if you want python 3.6, you can run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda create -n pytorch-example python=3.6
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-5-install-packages&#34;&gt;Step 5: Install packages&lt;/h3&gt;
&lt;p&gt;Now you can install packages as usual. For example, to install PyTorch, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure that the cuda version in the url matches the cuda version of the container.&lt;/p&gt;
&lt;p&gt;In each individual case, please follow the instructions from your project to install the required packages.&lt;/p&gt;
&lt;h3 id=&#34;step-6-end&#34;&gt;Step 6: End&lt;/h3&gt;
&lt;p&gt;After you have installed all the packages you need, simply exit the container by running &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-all-in-one-script&#34;&gt;üöÄ All-in-one script&lt;/h1&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-next-step&#34;&gt;‚è≠Ô∏è Next step&lt;/h1&gt;
&lt;p&gt;Since we installed all packages in the overlay image, we also need to run code inside the container when we submit a job. This will be covered in the future posts.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;related-posts&#34;&gt;Related posts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Previous: &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPC Part 2: Data management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>HPC Data Management</title>
      <link>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/</link>
      <pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/</guid>
      <description>&lt;p&gt;HPC-part-2&lt;/p&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;The post introduces the data management system on HPC. The &lt;strong&gt;main purpose&lt;/strong&gt; of this is to provide a foundation for future posts about setting up environments, managing data, and running jobs on HPC.&lt;/p&gt;
&lt;p&gt;There are four different types of file systems on HPC:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Home directory: &lt;code&gt;/home/&amp;lt;NetID&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Scratch directory: &lt;code&gt;/scratch/&amp;lt;NetID&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Archive directory: &lt;code&gt;/archive/&amp;lt;NetID&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Vast directory: &lt;code&gt;/vast/&amp;lt;NetID&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of them has different purposes and limitations. After you login to HPC, you can use the &lt;code&gt;myquota&lt;/code&gt; command to check your quota and usage on each file system. A sample output looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ myquota
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Hostname: log-1 at Sun Mar 21 21:59:08 EDT 2021
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Filesystem   Environment   Backed up?   Allocation       Current Usage
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Space        Variable      /Flushed?    Space / Files    Space(%) / Files(%)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/home        $HOME         Yes/No       50.0GB/30.0K       8.96GB(17.91%)/33000(110.00%)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/scratch     $SCRATCH      No/Yes        5.0TB/1.0M        811.09GB(15.84%)/2437(0.24%)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/archive     $ARCHIVE      Yes/No        2.0TB/20.0K       0.00GB(0.00%)/1(0.00%)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/vast        $VAST         No/Yes        2.0TB/5.0M        0.00GB(0.00%)/1(0.00%)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: As you can see, there are two types of limitations for each file system: &lt;em&gt;space&lt;/em&gt; and &lt;em&gt;files&lt;/em&gt; (also known as &lt;em&gt;inodes&lt;/em&gt;). The former is the total amount of space you can use on the file system and the latter is the total number of files you can store on the file system.&lt;/p&gt;
&lt;p&gt;The existence of file limitation is part of the reason why we need all these best practices in the first place.&lt;/p&gt;
&lt;p&gt;In the following sections, we will go through each file system in detail.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-home-directory&#34;&gt;üè† Home directory&lt;/h1&gt;
&lt;p&gt;The home directory is the default directory when you log in to HPC. As shown in the output above, the 50GB/30k limitation is quite small. Therefore, you are not recommended to store anything here.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-scratch-directory&#34;&gt;üìù Scratch directory&lt;/h1&gt;
&lt;p&gt;The scratch directory is the place where you will most play with. It is a temporary storage space for your data and jobs. The 5TB/1M limitation is enough for you to store &lt;strong&gt;almost&lt;/strong&gt; everythin you need.&lt;/p&gt;
&lt;p&gt;Note that this directory is &lt;strong&gt;flushed&lt;/strong&gt;, meaning any inactive files will be deleted after 60 days. When some of your files are about to be flushed, you will receive an email notification.&lt;/p&gt;
&lt;p&gt;However, the 1M file limitation is still relatively small, especially for modern datasets that usually contain large number of small files. We will cover details about this in the section for the &lt;code&gt;/vast&lt;/code&gt; directory.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-archive-directory&#34;&gt;üì¶ Archive directory&lt;/h1&gt;
&lt;p&gt;Like the home directoy, the archive directory is also a permanent storage space. However, it is not accessible from the computing nodes. Therefore, it&amp;rsquo;s recommended to use this directory only for &lt;strong&gt;archive&lt;/strong&gt; purpose.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-vast-directory&#34;&gt;üìÅ Vast directory&lt;/h1&gt;
&lt;p&gt;The vast directory is an &lt;a href=&#34;https://vastdata.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;all-flash&lt;/a&gt; file system that is optimized for computational workloads with high I/O rates.&lt;/p&gt;
&lt;p&gt;As mentioned above, as the vast directory has much larger inode limitation, it is recommended to store datasets that contain large number of small files here.&lt;/p&gt;
&lt;p&gt;We will discuss more about the best practices for large number of small files in future posts.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;related-posts&#34;&gt;Related posts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Previous: &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/access_to_hpc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPC Part 1: Access to HPC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Next: &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-env-setup/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPC Part 3: HPC Environment Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Accessing HPC</title>
      <link>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-access/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-access/</guid>
      <description>&lt;p&gt;HPC-part-1&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NYU High Performance Computing (NYU HPC) provides access to state of the art supercomputer hardware and cloud services to eligible faculty and students across all of NYU.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Most of the blog is adapted from &lt;a href=&#34;https://sites.google.com/nyu.edu/nyu-hpc/accessing-hpc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-getting-started&#34;&gt;üé¨ Getting started&lt;/h1&gt;
&lt;p&gt;Before accessing the HPC cluster, you need to have an account. If you don&amp;rsquo;t have one yet, please obtain one by following &lt;strong&gt;step 1&lt;/strong&gt; and &lt;strong&gt;step 2&lt;/strong&gt; in &lt;a href=&#34;https://www.nyu.edu/life/information-technology/research-computing-services/high-performance-computing/high-performance-computing-nyu-it/hpc-accounts-and-eligibility.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-network-configurations&#34;&gt;‚öôÔ∏è Network configurations&lt;/h1&gt;
&lt;p&gt;Now that you should have an HPC account, we need to configure your network settings to access the HPC cluster.&lt;/p&gt;
&lt;p&gt;If you are on the NYU network, you can access the HPC cluster directly without any extra configurations.&lt;/p&gt;
&lt;p&gt;Otherwise, you have two options to access the HPC cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NYU VPN&lt;/strong&gt;: You can use the &lt;a href=&#34;https://www.nyu.edu/life/information-technology/infrastructure/network-services/vpn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NYU VPN&lt;/a&gt; to connect to the NYU network. This is often default option for most users.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HPC gateway&lt;/strong&gt;: If in some cases you cannot use the NYU VPN, you can use the HPC gateway &lt;code&gt;gw.hpc.nyu.edu&lt;/code&gt; (example below) to connect. Note that it is only a portal-like server, and you cannot run any jobs on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h1 id=&#34;-command-line&#34;&gt;Ôºû Command line&lt;/h1&gt;
&lt;h3 id=&#34;1-gateway&#34;&gt;1. Gateway&lt;/h3&gt;
&lt;p&gt;(Ignore this step if you are using NYU VPN or on the NYU network)&lt;/p&gt;
&lt;p&gt;Connect to the HPC gateway using SSH:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh &amp;lt;NetID&amp;gt;@gw.hpc.nyu.edu
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2-ssh&#34;&gt;2. SSH&lt;/h3&gt;
&lt;p&gt;Connect to the HPC cluster using SSH:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh &amp;lt;NetID&amp;gt;@greene.hpc.nyu.edu
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;3-save-password&#34;&gt;3. Save password&lt;/h3&gt;
&lt;p&gt;(Optional)
Once you logged in to HPC, you can set up an SSH key to avoid typing your password every time you log in.&lt;/p&gt;
&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Only do this on your trusted computer&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, generate a SSH key pair on your local machine (disconnecting from HPC):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh-keygen -t rsa
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, copy the public key to the HPC cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh-copy-id &amp;lt;NetID&amp;gt;@greene.hpc.nyu.edu
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now you can log in to HPC without typing your password.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;-editor&#34;&gt;üìù Editor&lt;/h1&gt;
&lt;p&gt;This section is optional but highly recommended. You can use any editor you like to edit scripts on HPC, here I recommend using &lt;strong&gt;VS Code&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;1-install&#34;&gt;1. Install&lt;/h3&gt;
&lt;p&gt;Download and install VS Code from &lt;a href=&#34;https://code.visualstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;2-install-remote---ssh-extension&#34;&gt;2. Install Remote - SSH extension&lt;/h3&gt;
&lt;p&gt;After open VS Code, search for &lt;code&gt;SSH&lt;/code&gt; in the extension market and install &lt;code&gt;Remote - SSH&lt;/code&gt;.
















&lt;figure  id=&#34;figure-remote---ssh-extension&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ssh&#34; srcset=&#34;
               /The-Best-Practices/hpc/hpc-access/ssh_hu7dde6020f4f295e840f5b2b254d5f83a_85934_946abdcbced6f62e08bc3aab36f80a8d.webp 400w,
               /The-Best-Practices/hpc/hpc-access/ssh_hu7dde6020f4f295e840f5b2b254d5f83a_85934_ec269ec80f5c79271ff4ea28e56d0487.webp 760w,
               /The-Best-Practices/hpc/hpc-access/ssh_hu7dde6020f4f295e840f5b2b254d5f83a_85934_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://gaaaavin.github.io/The-Best-Practices/The-Best-Practices/hpc/hpc-access/ssh_hu7dde6020f4f295e840f5b2b254d5f83a_85934_946abdcbced6f62e08bc3aab36f80a8d.webp&#34;
               width=&#34;760&#34;
               height=&#34;155&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Remote - SSH extension
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;3-connect-to-hpc&#34;&gt;3. Connect to HPC&lt;/h3&gt;
&lt;p&gt;Click the button in the bottom left corner of VS Code and select &lt;code&gt;Connect to Host...&lt;/code&gt;. Then, follow the instructions to connect to HPC.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Now you should be able to access the HPC cluster and edit scripts on it.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id=&#34;related-posts&#34;&gt;Related posts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;a href=&#34;https://gaaaavin.github.io/The-Best-Practices/hpc/hpc-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPC Part 2: Data management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
